# STP Speech plugins

The Sketch-thru-Plan (STP) recognizer can employ transcribed speech generated by potentially different recognizers. To promote code reuse and make it possible to more easily swap recognizers, the functionality should be packaged as a plugin that conforms to a well-known interface: [`IStpSpeechRecognizer`](interfaces/IStpSpeechRecognizer). 

## Speech recognition strategies

STP operates by combining multiple types of user input (or *modalities*) such as sketches and speech, and producing interpretations that represent the combination of these modalities. Users naturally produce the sketches and the speech that it relates to in close temporal proximity: a sketch and the speech it relates to are produced in general within a few seconds of each other.

Taking advantage of this natural style, STP uses sketch start events as anchors points around which speech produced within a window of a few seconds is considered for combined interpretation. A few strategies for handling speech interpretation are therefore possible:

1. "One-shot" recognition - activate recognition at the start of each stroke, and listen for a period of time - this is a simple, but effective strategy. Users need to be mindful in this case that whatever they speak before the stroke is started will not be captured by the system. This is the strategy used in the [quickstarts](../../quickstart/)

1. "While sketching" - recognize speech while the user is sketching, activating it at the moment the sketch starts, and ending a few seconds after the sketching ends. This is the strategy used in the [samples](../../samples/)

1. Continuous recognition - speech can also be transcribed/recognized continuously, and sent over to STP for consideration. STP has mechanisms to just consider speech that might be relevant. This is approach maximizes the capture of users' speech, but comes at additional computational costs because of the constant transcription of an open microphone

1. Capture 2 seconds of audio before the start of each stroke - ideally, audio buffers would be accessed to extract a limited amount of audio just before the start of a stroke, so that no part of the speech is lost, even if users start to talk a bit before sketching. This depends on specific audio techniques that are out of scope of the present discussion

## Implemented Speech plugins

The [`azurespeech-plugin`](azurespeech-plugin) plugin implements is an implementation using the Microsoft Cognitive Services Speech to Text. It implements the strategies 1 and 2 and 3, supporting recognition once at a time, as well as over a period of time that can be restricted to the duration of the sketching (strategy 2), or some other period of time determined by the client app (strategy 3). 

### "One-shot" recognition

To employ this approach, invoke the `recognizeOnce()` method whenever the user starts to sketch (usually within a mouse/pen down even handler). The results are returned (via a Promise) when the recognition has been achieved. Null is returned if there is no successful recognition within a few seconds after the method invocation.

```javascript
/**
 * Activate the microphone and attempt to recognize speech in the next few seconds
 * Ideally, the recognition would include 2s of audio _before_ the call, drawing from some buffer
 * @param maxRetries - Number of time to retry before returning an error
 * @returns Recognized items/hypotheses, or null if nothing was recognized
 */
recognizeOnce(maxRetries?: number): Promise<ISpeechRecoResult | null>;
```

### "While sketching" recognition

This approach requires multiple methods and event handlers. Typically `startRecognizing()` is invoked when the user starts sketching (in a mouse/pen down event handler), and `stopRecognizing()` is called when sketching is completed (in a mouse/pen up or sketch completed event handler). To give users an opportunity to speak right after completing the sketch, a timeout parameter can be provided to `stopRecognizing()`. A timeout of about 5 seconds is sufficient in most cases.

```javascript
/**
 * Start the recognition process. Intermediate results are returned via the onRecognizing event;
 * final results (phrase) are returned via the onRecognized event
 * The expectation is that speech recognition is started at the beginning of a sketch, and stopped
 * sometime after the end of the sketch
 */
startRecognizing(): void;

/**
 * Stop the recognition process. Is normally called at the end of a sketch action
 * @param wait Time in seconds to wait before stopping recognition
 */
stopRecognizing(wait?: number): void;
```

Recognition proceeds asynchronously. The client app is required to provide event handlers to be notified of the recognition results. The main event is `onRecognized`, which is called when a full phrase has been interpreted. The optional `onRecognizing` returns incremental hypotheses, as speech is tentatively interpreted. It might be useful to provide feedback to users on longer input. Finally, `onError` is invoked whenever there is a problem that causes the recognition to be interrupted/aborted. 

In most cases, it is convenient to invoke `stopRecognizing()` when the first recognized phrase is returned via `onRecognized`. If that is not done, multiple additional calls to `onRecognized` may occur, as the user might continue to speak. Normally one sketch gesture is associated with a single recognition, so the additional input will likely be unrelated.

```javascript
/**
 * Event handler invoked whenever the recognizer has a complete phrase to return
 */
onRecognized: ((result: ISpeechRecoResult | null) => void) | undefined;

/**
 * Optional event handler invoked whenever the recognizer has a partial recognition available
 */
onRecognizing: ((snippet: string) => void) | undefined;

/**
 * Optional event handler invoked when there is a recognition error
 */
onError: ((error: Error) => void) | undefined;
```

### Continuous recognition

The methods and events used to support "while sketching" recognition as described above can also be used to handle continuous recognition. Reco could be started for example as a response to some user action (a button click?). As the user speaks, `onRecognized` is invoked repeatedly, and the phrases could be sent to STP independently of the sketching, which could be sent asynchronously as they are completed independently.

This strategy could let users speak before placing the sketches, which some users might prefer, but will likely result in a higher number of potentially irrelevant speech being processed, if capturing user speech that is not directed to STP. Keeping the microphone open for longer periods of time may also not be as robust.

## Recognition results

Successful recognition results in multiple hypothesis being returned, either by the "one-shot" `recognizeOnce()` or via the `onRecognized` event. The structure of these results is described by the `ISpeechRecoResult` and associated `ISpeechRecoItem` interfaces:

```javascript
/**
 * Speech recognition results
 */
export interface ISpeechRecoResult {
/**
 * Speech recognition hypothesis
 */
results: ISpeechRecoItem[];
/**
 * Time speech started
 */
startTime: Date;
/**
 * Time speech ended
 */
endTime: Date;
}
/**
 * Recognition hypotheses
 */
export interface ISpeechRecoItem {
/**
 * Transcribed speech text
 */
text: string;
/**
 * Likelihood/confidence of the interpretation
 */
confidence: number;
}
```