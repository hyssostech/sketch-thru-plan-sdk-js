# STP Speech plugins

The Sketch-thru-Plan (STP) recognizer can employ transcribed speech generated by potentially different recognizers. To promote code reuse and make it possible to more easily swap recognizers, the functionality should be packaged as a plugin that conforms to a well-known interface. 

## STP speech recognition interface

STP's required functionality is encapsulated in the standard [`IStpSpeechRecognizer`](interfaces/IStpSpeechRecognizer) interface:

```javascript
/**
 * Stp interface to speech recognition engine
 * @interface
 */
export interface ISpeechRecognizer {
    /**
     * Start recognizing speech
     * At a minimum the speech in the next few seconds should be interpreted. Ideally, the recognition
     * would include 2s of audio _before_ the call, drawing from some buffer
     * 
     * @param maxRetries - Optional number of times to retry the operation
     * @returns Recognized items/hypotheses or null if nothing was recognized (no speech during the time the mike remains active)
     */
    recognize(maxRetries?: number): Promise<ISpeechRecoResult | null>;
}

/**
 * Speech recognition results
 */
export interface ISpeechRecoResult {
    /**
     * Speech recognition hypothesis
     */
    results: ISpeechRecoItem[];
    /**
     * Time speech started
     */
    startTime: Date;
    /**
     * Time speech ended
     */
    endTime: Date;
}
/**
 * Recognition hypotheses
 */
export interface ISpeechRecoItem {
    /**
     * Transcribed speech text
     */
    text: string;
    /**
     * Likelihood/confidence of the interpretation
     */
    confidence: number;
}
```

## Speech recognition strategies

STP operates by combining multiple types of user input (or *modalities*) such as sketches and speech, and producing interpretations that represent the combination of these modalities. Users naturally produce the sketches and the speech that it relates to in close temporal proximity: a sketch and the speech it relates to are produced in general within a few seconds of each other.

Taking advantage of this natural style, STP uses sketch start events as anchors points around which speech produced within a window of a few seconds is considered for combined interpretation. A few strategies for handling speech interpretation are therefore possible:

1. Activate recognition at the start of each stroke - this is a simple, but effective strategy. Users need to be mindful in this case that whatever they speak before the stroke is started will not be captured by the system. This is the strategy used in the [quickstarts](../../quickstart/)

1. Recognize speech while the user is sketching, activating it at the moment the sketch starts, and ending a few seconds after the sketching ends. This is the strategy used in the [samples](../../samples/)

1. Continuous recognition - speech can also be transcribed/recognized continuously, and sent over to STP for consideration. STP has mechanisms to just consider speech that might be relevant. This is approach maximizes the capture of users' speech, but comes at additional computational costs because of the constant transcription of an open microphone

1. Capture 2 seconds of audio before the start of each stroke - ideally, audio buffers would be accessed to extract a limited amount of audio just before the start of a stroke, so that no part of the speech is lost, even if users start to talk a bit before sketching. This depends on specific audio techniques that are out of scope of the present discussion

## Implemented Speech plugins

The [`azurespeech-plugin`](azurespeech-plugin) plugin implements is an implementation using the Microsoft Cognitive Services Speech to Text. It implements the strategies 1 and 2 and 3, supporting recognition once at a time, as well as over a period of time that can be restricted to the duration of the sketching (strategy 2), or some other period of time determined by the client app (strategy 3). 
